{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "sys.path.append('D:\\Machine Learning\\Paper\\Object Detection\\YOLO\\pytorch-yolo2-master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import dataset\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "os.chdir('D:\\Machine Learning\\Paper\\Object Detection\\YOLO\\pytorch-yolo2-master')\n",
    "from utils import *\n",
    "from cfg import parse_cfg\n",
    "from region_loss import RegionLoss\n",
    "from darknet import Darknet\n",
    "from models.tiny_yolo import TinyYoloNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "datacfg       = 'cfg/voc.data'\n",
    "cfgfile       = 'cfg/yolo-voc.cfg'\n",
    "weightfile    = 'darknet19_448.conv.23'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_options  = read_data_cfg(datacfg)\n",
    "net_options   = parse_cfg(cfgfile)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'backup': 'backup',\n",
       " 'gpus': '0,1,2,3',\n",
       " 'names': 'data/voc.names',\n",
       " 'num_workers': '10',\n",
       " 'train': 'train.txt',\n",
       " 'valid': '2007_test.txt'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'angle': '0',\n",
       " 'batch': '32',\n",
       " 'burn_in': '1000',\n",
       " 'channels': '3',\n",
       " 'decay': '0.0005',\n",
       " 'exposure': '1.5',\n",
       " 'height': '416',\n",
       " 'hue': '.1',\n",
       " 'learning_rate': '0.001',\n",
       " 'max_batches': '80200',\n",
       " 'momentum': '0.9',\n",
       " 'policy': 'steps',\n",
       " 'saturation': '1.5',\n",
       " 'scales': '0.1,10,.1,.1',\n",
       " 'steps': '-1,500,40000,60000',\n",
       " 'subdivisions': '8',\n",
       " 'type': 'net',\n",
       " 'width': '416'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlist     = data_options['train']\n",
    "testlist      = data_options['valid']\n",
    "backupdir     = data_options['backup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train.txt'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples      = file_lines(trainlist)\n",
    "gpus          = data_options['gpus']  # e.g. 0,1,2,3\n",
    "ngpus         = 0\n",
    "num_workers   = int(data_options['num_workers'])\n",
    "\n",
    "batch_size    = int(net_options['batch'])\n",
    "max_batches   = int(net_options['max_batches'])\n",
    "learning_rate = float(net_options['learning_rate'])\n",
    "momentum      = float(net_options['momentum'])\n",
    "decay         = float(net_options['decay'])\n",
    "steps         = [float(step) for step in net_options['steps'].split(',')]\n",
    "scales        = [float(scale) for scale in net_options['scales'].split(',')]\n",
    "\n",
    "#Train parameters\n",
    "max_epochs    = max_batches*batch_size/nsamples+1\n",
    "use_cuda      = False\n",
    "seed          = int(time.time())\n",
    "eps           = 1e-5\n",
    "save_interval = 10  # epoches\n",
    "dot_interval  = 70  # batches\n",
    "\n",
    "# Test parameters\n",
    "conf_thresh   = 0.25\n",
    "nms_thresh    = 0.4\n",
    "iou_thresh    = 0.5\n",
    "\n",
    "if not os.path.exists(backupdir):\n",
    "    os.mkdir(backupdir)\n",
    "    \n",
    "###############\n",
    "torch.manual_seed(seed)\n",
    "if use_cuda:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = gpus\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "#Train parameters\n",
    "max_epochs    = 32 * max_batches*batch_size/nsamples+1\n",
    "use_cuda      = False\n",
    "seed          = int(time.time())\n",
    "eps           = 1e-5\n",
    "save_interval = 10  # epoches\n",
    "dot_interval  = 70  # batches\n",
    "\n",
    "# Test parameters\n",
    "conf_thresh   = 0.25\n",
    "nms_thresh    = 0.4\n",
    "iou_thresh    = 0.5\n",
    "\n",
    "if not os.path.exists(backupdir):\n",
    "    os.mkdir(backupdir)\n",
    "    \n",
    "###############\n",
    "torch.manual_seed(seed)\n",
    "if use_cuda:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = gpus\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet(cfgfile)\n",
    "region_loss = model.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Machine Learning\\Paper\\Object Detection\\YOLO\\pytorch-yolo2-master\\cfg.py:175: UserWarning: src is not broadcastable to dst, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n",
      "  conv_model.weight.data.copy_(torch.from_numpy(buf[start:start+num_w])); start = start + num_w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer     filters    size              input                output\n",
      "    0 conv     32  3 x 3 / 1   416 x 416 x   3   ->   416 x 416 x  32\n",
      "    1 max          2 x 2 / 2   416 x 416 x  32   ->   208 x 208 x  32\n",
      "    2 conv     64  3 x 3 / 1   208 x 208 x  32   ->   208 x 208 x  64\n",
      "    3 max          2 x 2 / 2   208 x 208 x  64   ->   104 x 104 x  64\n",
      "    4 conv    128  3 x 3 / 1   104 x 104 x  64   ->   104 x 104 x 128\n",
      "    5 conv     64  1 x 1 / 1   104 x 104 x 128   ->   104 x 104 x  64\n",
      "    6 conv    128  3 x 3 / 1   104 x 104 x  64   ->   104 x 104 x 128\n",
      "    7 max          2 x 2 / 2   104 x 104 x 128   ->    52 x  52 x 128\n",
      "    8 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256\n",
      "    9 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128\n",
      "   10 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256\n",
      "   11 max          2 x 2 / 2    52 x  52 x 256   ->    26 x  26 x 256\n",
      "   12 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512\n",
      "   13 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256\n",
      "   14 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512\n",
      "   15 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256\n",
      "   16 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512\n",
      "   17 max          2 x 2 / 2    26 x  26 x 512   ->    13 x  13 x 512\n",
      "   18 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024\n",
      "   19 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512\n",
      "   20 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024\n",
      "   21 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512\n",
      "   22 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024\n",
      "   23 conv   1024  3 x 3 / 1    13 x  13 x1024   ->    13 x  13 x1024\n",
      "   24 conv   1024  3 x 3 / 1    13 x  13 x1024   ->    13 x  13 x1024\n",
      "   25 route  16\n",
      "   26 conv     64  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x  64\n",
      "   27 reorg              / 2    26 x  26 x  64   ->    13 x  13 x 256\n",
      "   28 route  27 24\n",
      "   29 conv   1024  3 x 3 / 1    13 x  13 x1280   ->    13 x  13 x1024\n",
      "   30 conv    125  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 125\n",
      "   31 detection\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(weightfile)\n",
    "model.print_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_loss.seen  = model.seen\n",
    "processed_batches = model.seen/batch_size\n",
    "\n",
    "init_width        = model.width\n",
    "init_height       = model.height\n",
    "init_epoch        = model.seen/nsamples \n",
    "\n",
    "kwargs = {'num_workers': num_workers, 'pin_memory': True} if use_cuda else {}\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset.listDataset(testlist, shape=(init_width, init_height),\n",
    "                   shuffle=False,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                   ]), train=False),\n",
    "    batch_size=batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    if ngpus > 1:\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "    else:\n",
    "        model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = dict(model.named_parameters())\n",
    "params = []\n",
    "for key, value in params_dict.items():\n",
    "    if key.find('.bn') >= 0 or key.find('.bias') >= 0:\n",
    "        params += [{'params': [value], 'weight_decay': 0.0}]\n",
    "    else:\n",
    "        params += [{'params': [value], 'weight_decay': decay*batch_size}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate/batch_size, momentum=momentum, dampening=0, weight_decay=decay*batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, batch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = learning_rate\n",
    "    for i in range(len(steps)):\n",
    "        scale = scales[i] if i < len(scales) else 1\n",
    "        if batch >= steps[i]:\n",
    "            lr = lr * scale\n",
    "            if batch == steps[i]:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr/batch_size\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "global processed_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "if ngpus > 1:\n",
    "    cur_model = model.module\n",
    "else:\n",
    "    cur_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "dataset.listDataset(trainlist, shape=(init_width, init_height),\n",
    "   shuffle=True,\n",
    "   transform=transforms.Compose([\n",
    "   transforms.ToTensor(),\n",
    "   ]), \n",
    "   train=True, \n",
    "   seen=cur_model.seen,\n",
    "   batch_size=batch_size,\n",
    "   num_workers=num_workers),\n",
    "batch_size=batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = adjust_learning_rate(optimizer, processed_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-01 11:06:12 epoch 1, processed 16551 samples, lr 0.000100\n"
     ]
    }
   ],
   "source": [
    "logging('epoch %d, processed %d samples, lr %f' % (epoch, epoch * len(train_loader.dataset), lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "batch_idx, (data, target) = next(enumerate(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = Variable(data), Variable(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_loss.seen = region_loss.seen + data.data.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loss = region_loss(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "        #output : BxAs*(4+1+num_classes)*H*W\n",
    "        t0 = time.time()\n",
    "        nB = output.data.size(0)\n",
    "        nA = region_loss.num_anchors\n",
    "        nC = region_loss.num_classes\n",
    "        nH = output.data.size(2)\n",
    "        nW = output.data.size(3)\n",
    "        \"\"\"\n",
    "        nB,nA,nC,nH,nW\n",
    "        (32, 5, 20, 13, 13)\n",
    "        \"\"\"\n",
    "        # output.shape : [32, 125, 13, 13]\n",
    "        output   = output.view(nB, nA, (5+nC), nH, nW) # [32, 5, 25, 13, 13]\n",
    "        x = F.sigmoid(output.index_select(2, Variable(torch.LongTensor([0]))).view(nB, nA, nH, nW))\n",
    "        \"\"\"\n",
    "        index_select : http://pytorch.org/docs/master/torch.html?highlight=index_select#torch.index_select\n",
    "        相当于tf.gather\n",
    "        x.shape = [32, 5, 13, 13]\n",
    "        \"\"\"\n",
    "        y = F.sigmoid(output.index_select(2, Variable(torch.LongTensor([1]))).view(nB, nA, nH, nW))\n",
    "        # y.shape = [32, 5, 13, 13]\n",
    "        w = output.index_select(2, Variable(torch.LongTensor([2]))).view(nB, nA, nH, nW)\n",
    "        # w.shape = [32, 5, 13, 13]\n",
    "        h = output.index_select(2, Variable(torch.LongTensor([3]))).view(nB, nA, nH, nW)\n",
    "        # h.shape = [32, 5, 13, 13]\n",
    "        conf = F.sigmoid(output.index_select(2, Variable(torch.LongTensor([4]))).view(nB, nA, nH, nW))\n",
    "        # conf.shape = [32, 5, 13, 13]\n",
    "        cls = output.index_select(2, Variable(torch.linspace(5,5+nC-1,nC).long()))\n",
    "        # torch.linspace(5,5+nC-1,nC).long() = [5,6,7.....,24] 取剩下20个输出 cls.shape = [32, 5, 20, 13, 13]\n",
    "        cls = cls.view(nB*nA, nC, nH*nW).transpose(1,2).contiguous().view(nB*nA*nH*nW, nC)\n",
    "        \"\"\"\n",
    "        注意这样和直接.view(nB*nA*nH*nW, nC)得到的结果是不同的\n",
    "        这样操作的结果是以nC维度为轴，做的转换\n",
    "        cls.shape = [27040, 20]\n",
    "        \"\"\"\n",
    "        t1 = time.time()\n",
    "\n",
    "        pred_boxes = torch.FloatTensor(4, nB*nA*nH*nW)\n",
    "        # pred_boxes.shape = [4, 27040]\n",
    "        grid_x = torch.linspace(0, nW-1, nW).repeat(nH,1).repeat(nB*nA, 1, 1).view(nB*nA*nH*nW)\n",
    "        grid_y = torch.linspace(0, nH-1, nH).repeat(nW,1).t().repeat(nB*nA, 1, 1).view(nB*nA*nH*nW)\n",
    "        \"\"\"\n",
    "        grid_x.shape = [27040]\n",
    "        grid_y.shape = [27040]\n",
    "        生成了0,1,2,3,4,5,6,7,8,9,10,11,12,0,1,2,3,,,,,,,,,(不停的重复13*batch*5次)\n",
    "        \"\"\"\n",
    "        anchor_w = torch.Tensor(region_loss.anchors).view(nA, region_loss.anchor_step).index_select(1, torch.LongTensor([0]))\n",
    "        anchor_h = torch.Tensor(region_loss.anchors).view(nA, region_loss.anchor_step).index_select(1, torch.LongTensor([1]))\n",
    "        \"\"\"\n",
    "        把anchors分成两拨，实际上是square的两条边的大小\n",
    "        region_loss.anchors:10个items的list\n",
    "        [1.3221,\n",
    "         1.73145,\n",
    "         3.19275,\n",
    "         4.00944,\n",
    "         5.05587,\n",
    "         8.09892,\n",
    "         9.47112,\n",
    "         4.84053,\n",
    "         11.2364,\n",
    "         10.0071]\n",
    "         anchor_w : shape:[5,1]\n",
    "          1.3221\n",
    "          3.1927\n",
    "          5.0559\n",
    "          9.4711\n",
    "         11.2364\n",
    "         anchor_h : shape:[5,1]\n",
    "          1.7314\n",
    "          4.0094\n",
    "          8.0989\n",
    "          4.8405\n",
    "         10.0071\n",
    "        \"\"\"\n",
    "        anchor_w = anchor_w.repeat(nB, 1).repeat(1, 1, nH*nW).view(nB*nA*nH*nW)\n",
    "        anchor_h = anchor_h.repeat(nB, 1).repeat(1, 1, nH*nW).view(nB*nA*nH*nW)\n",
    "        # 把[5,1] repeat很多遍，尺寸[27040]\n",
    "        # pred_boxes[0] = x.data + grid_x\n",
    "        # pred_boxes[1] = y.data + grid_y\n",
    "        # pred_boxes[2] = torch.exp(w.data) * anchor_w\n",
    "        # pred_boxes[3] = torch.exp(h.data) * anchor_h\n",
    "\n",
    "        pred_boxes[0] = x.data.view([-1]) + grid_x\n",
    "        pred_boxes[1] = y.data.view([-1]) + grid_y\n",
    "        pred_boxes[2] = torch.exp(w.data.view([-1])) * anchor_w\n",
    "        pred_boxes[3] = torch.exp(h.data.view([-1])) * anchor_h\n",
    "        \"\"\"\n",
    "        上面这个写法太不规范，改了一下，最终输出结果是一样的\n",
    "        可以这样理解这个操作,先看一下\n",
    "        grid_y[13*13:13*13*2] 0,1,2,3,4,5,6,7,8,9,10,11,12,0,1,2,3,4,,,,,\n",
    "        grid_x[13*13:13*13*2] 0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,,,,\n",
    "        grid_x和grid_y拼起来就是每13*13一段，拼起来正好是遍历了13*13棋盘中的每个点\n",
    "        anchor_h[13*13:13*13*2]和anchor_h[13*13:13*13*2]里面的值都只有一个(h和w还是不同的)\n",
    "        一个h和一个w合起来就是一个anchor的尺寸\n",
    "        每13*13个中心点共一个anchor尺寸\n",
    "        总共有batch * num_anchors = 32 * 5 = 160个尺寸\n",
    "        所以和起来是遍历了每一张图片的13*13个中心点中的每一个点上面的20个预测类别的5种anchor尺寸，\n",
    "        x和y是输出的中点，区间[0,1](经过sigmoid)\n",
    "        可以看做是把每一张图划分成13*13个小格，x和y就是该小格预测出来的目标的中心点相对于小格内的坐标\n",
    "        但是这个奇怪的anchor尺寸是怎么来的 ？\n",
    "        难道是论文里Dimension Clusters那一段说的通过k-means从dataset里面算出来的 ？感觉也不是\n",
    "        \"\"\"\n",
    "\n",
    "        pred_boxes = convert2cpu(pred_boxes.transpose(0,1).contiguous().view(-1,4))\n",
    "        # 转到cpu也是为了截断梯度 ？\n",
    "        t2 = time.time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
